{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad276f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# import glob\n",
    "# import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9820b98",
   "metadata": {},
   "source": [
    "# Get max, int, min channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51496335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channels(image):\n",
    "    b,g,r=cv2.split(image)\n",
    "    b_mean=np.mean(b)\n",
    "    g_mean=np.mean(g)\n",
    "    r_mean=np.mean(r)\n",
    "    \n",
    "    max_val=max(b_mean,g_mean,r_mean)\n",
    "    min_val=min(b_mean,g_mean,r_mean)\n",
    "    \n",
    "    if(max_val==b_mean):\n",
    "        i_max=b\n",
    "        print('blue')\n",
    "        if(min_val==r_mean):\n",
    "            i_min=r\n",
    "        else:\n",
    "            i_min=g\n",
    "    elif(max_val==g_mean):\n",
    "        i_max=g\n",
    "        print('green')\n",
    "        if(min_val==r_mean):\n",
    "            i_min=r\n",
    "        else:\n",
    "            i_min=b\n",
    "    else:\n",
    "        i_max=r\n",
    "        print('red')\n",
    "        if(min_val==b_mean):\n",
    "            i_min=b\n",
    "        else:\n",
    "            i_min=g\n",
    "    \n",
    "    if((max_val==g_mean and min_val==r_mean) or (min_val==g_mean and max_val==r_mean)):\n",
    "        i_int=b\n",
    "    elif((max_val==b_mean and min_val==r_mean) or (min_val==b_mean and max_val==r_mean)):\n",
    "        i_int=g\n",
    "    else:\n",
    "        i_int=r\n",
    "    \n",
    "    return(i_max,i_int,i_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2db21",
   "metadata": {},
   "source": [
    "# Color Correction Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9aa2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weights(i_max,i_int,i_min):\n",
    " \n",
    "    att_int=i_max-i_int\n",
    "    att_min=i_max-i_min\n",
    "    att_max=np.absolute(i_max-i_int-i_min)\n",
    "    \n",
    "    return(att_max,att_int,att_min)\n",
    "    #print(i_max,i_min,i_int)\n",
    "    #print(np.mean(b))\n",
    "    #print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9524ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_attention_weights(image,attention_weights,i_max,i_int,i_min):\n",
    "    \"\"\"\n",
    "    Attach attention weights to color channels of an image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image in BGR format.\n",
    "        attention_weights (list): List of attention weights for each channel.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with attention weights applied to color channels.\n",
    "    \"\"\"\n",
    "    if len(attention_weights) != 3:\n",
    "        raise ValueError(\"Attention weights should be a list of length 3 (for BGR channels).\")\n",
    "\n",
    "    # Split the input image into BGR channels\n",
    "    b, g, r = cv2.split(image)\n",
    "    \n",
    "\n",
    "    # Apply attention weights to each channel\n",
    "#     weighted_b = b * attention_weights[0]\n",
    "#     weighted_g = g * attention_weights[1]\n",
    "#     weighted_r = r * attention_weights[2]\n",
    "\n",
    "    weighted_i_int=i_int+(attention_weights[1]*i_max)\n",
    "    weighted_i_min=i_min+(attention_weights[2]*i_max)\n",
    "    weighted_i_max=i_max#-(attention_weights[0]*intensities[0])\n",
    "    \n",
    "    if(np.array_equal(i_max,b)):\n",
    "        weighted_b=weighted_i_max\n",
    "        if(np.array_equal(i_int,r)):\n",
    "            weighted_r=weighted_i_int\n",
    "            weighted_g=weighted_i_min\n",
    "        else:\n",
    "            weighted_g=weighted_i_int\n",
    "            weighted_r=weighted_i_min\n",
    "    elif(np.array_equal(i_max,g)):\n",
    "        weighted_g=weighted_i_max\n",
    "        if(np.array_equal(i_int,r)):\n",
    "            weighted_r=weighted_i_int\n",
    "            weighted_b=weighted_i_min\n",
    "        else:\n",
    "            weighted_b=weighted_i_int\n",
    "            weighted_r=weighted_i_min\n",
    "    else:\n",
    "        weighted_r=weighted_i_max\n",
    "        if(np.array_equal(i_int,g)):\n",
    "            weighted_g=weighted_i_int\n",
    "            weighted_b=weighted_i_min\n",
    "        else:\n",
    "            weighted_b=weighted_i_int\n",
    "            weighted_g=weighted_i_min\n",
    "        \n",
    "    \n",
    "\n",
    "    # Merge the modified channels back into an image\n",
    "    weighted_image = cv2.merge([weighted_b, weighted_g, weighted_r])\n",
    "    \n",
    "    #print(weighted_image.shape)\n",
    "    \n",
    "    Omax=np.max(weighted_image)\n",
    "    Omin=np.min(weighted_image)\n",
    "    \n",
    "    Icr_max=Omin+(weighted_i_max-np.min(weighted_i_max))*((Omax-Omin)/(np.max(weighted_i_max)-np.min(weighted_i_max)))\n",
    "    Icr_int=Omin+(weighted_i_int-np.min(weighted_i_int))*((Omax-Omin)/(np.max(weighted_i_int)-np.min(weighted_i_int)))\n",
    "    Icr_min=Omin+(weighted_i_min-np.min(weighted_i_min))*((Omax-Omin)/(np.max(weighted_i_min)-np.min(weighted_i_min)))\n",
    "    \n",
    "    \n",
    "    if(np.array_equal(i_max,b)):\n",
    "        weighted_b=Icr_max\n",
    "        if(np.array_equal(i_int,r)):\n",
    "            weighted_r=Icr_int\n",
    "            weighted_g=Icr_min\n",
    "        else:\n",
    "            weighted_g=Icr_int\n",
    "            weighted_r=Icr_min\n",
    "    elif(np.array_equal(i_max,g)):\n",
    "        weighted_g=Icr_max\n",
    "        if(np.array_equal(i_int,r)):\n",
    "            weighted_r=Icr_int\n",
    "            weighted_b=Icr_min\n",
    "        else:\n",
    "            weighted_b=Icr_int\n",
    "            weighted_r=Icr_min\n",
    "    else:\n",
    "        weighted_r=Icr_max\n",
    "        if(np.array_equal(i_int,g)):\n",
    "            weighted_g=Icr_int\n",
    "            weighted_b=Icr_min\n",
    "        else:\n",
    "            weighted_b=Icr_int\n",
    "            weighted_g=Icr_min\n",
    "    # Clip the values to ensure they are within the valid range [0, 255]\n",
    "    \n",
    "    corrected_image = cv2.merge([weighted_b, weighted_g, weighted_r])\n",
    "    final_image = np.clip(corrected_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return (weighted_image,final_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c186c5",
   "metadata": {},
   "source": [
    "# Remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522e7a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(image,kernel_size):\n",
    "    \"\"\"\n",
    "    Remove noise from an input image using Gaussian smoothing.\n",
    "\n",
    "    Args:\n",
    "        input_image_path (str): Path to the input image.\n",
    "        output_image_path (str): Path to save the denoised image.\n",
    "        kernel_size (int): Size of the Gaussian kernel for smoothing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Read the input image\n",
    "\n",
    "    # Apply Gaussian smoothing to remove noise\n",
    "    denoised_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
    "\n",
    "    # Save the denoised image\n",
    "    #cv2.imwrite(output_image_path, denoised_image)\n",
    "    return denoised_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d7444",
   "metadata": {},
   "source": [
    "# Histogram maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f6491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(image):\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    #grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    b,g,r=cv2.split(image)\n",
    "\n",
    "    # Calculate the histogram of the grayscale image\n",
    "    histblue = cv2.calcHist(b, [0], None, [256], [0, 256])\n",
    "    histgreen = cv2.calcHist(g, [0], None, [256], [0, 256])\n",
    "    histred = cv2.calcHist(r, [0], None, [256], [0, 256])\n",
    "    \n",
    "    figure, axis = plt.subplots(2, 2)\n",
    "    # Plot the histogram\n",
    "    axis[0,0].plot(histblue)\n",
    "    #plt.show()\n",
    "    axis[0,1].plot(histgreen)\n",
    "    #plt.show()\n",
    "    axis[1,0].plot(histred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d0410",
   "metadata": {},
   "source": [
    "# Rayleigh Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8357244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rayleigh_channel(icr,img,min_lim,max_lim,i_min,alpha):\n",
    "    i_c_min=np.min(img)\n",
    "    i_c_max=np.max(img)\n",
    "    \n",
    "    if(i_c_min>min_lim):\n",
    "        O_min=i_c_min\n",
    "    else:\n",
    "        O_min=min_lim\n",
    "        \n",
    "    if(i_c_max>max_lim):\n",
    "        O_max=i_c_max\n",
    "    else:\n",
    "        O_max=max_lim\n",
    "    \n",
    "    statement=(icr-i_min)*((O_max-O_min)/(np.max(i_min)-np.min(i_min)))\n",
    "    \n",
    "    internal=(O_min+statement)/(2*alpha*alpha)\n",
    "    exponent=np.exp(-1*internal*internal)\n",
    "    \n",
    "    external=statement/(alpha*alpha)\n",
    "    \n",
    "    irw=external*exponent\n",
    "    \n",
    "    return irw    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c81605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rayleigh_dist(icr,image,i_min,alpha):\n",
    "    b,g,r=cv2.split(image)\n",
    "    \n",
    "    minimum_bgr=min(np.min(b),np.min(g),np.min(r))\n",
    "    \n",
    "    three_point_five=0.035*minimum_bgr\n",
    "    ninesix_point_five=0.965*minimum_bgr\n",
    "    \n",
    "    bcr,gcr,rcr=cv2.split(icr)\n",
    "    \n",
    "#     mini=np.min(i_min)\n",
    "#     maxi=np.max(i_min)\n",
    "    \n",
    "#     if(mini>three_point_five):\n",
    "#         O_min=mini\n",
    "#     else:\n",
    "#         O_min=three_point_five\n",
    "        \n",
    "#     if(maxi<ninesix_point_five):\n",
    "#         O_max=maxi\n",
    "#     else:\n",
    "#         O_max=ninesix_point_five\n",
    "    \n",
    "#     #internal=np.power(((O_min+(icr-i_min)*((O_max-O_min)/(np.max(i_min)-np.min(i_min))))/(2*alpha*alpha)),2)\n",
    "    \n",
    "#     print(icr.shape)\n",
    "#     print(i_min.shape)\n",
    "    \n",
    "#     statement=(icr-i_min)*((O_max-O_min)/(np.max(i_min)-np.min(i_min)))\n",
    "    \n",
    "#     internal=(O_min+statement)/(2*alpha*alpha)\n",
    "#     exponent=np.exp(-1*internal*internal)\n",
    "    \n",
    "#     external=statement/(alpha*alpha)\n",
    "    \n",
    "#     irw=external*exponent\n",
    "\n",
    "    b_rw=rayleigh_channel(bcr,b,three_point_five,ninesix_point_five,i_min,alpha)\n",
    "    g_rw=rayleigh_channel(gcr,g,three_point_five,ninesix_point_five,i_min,alpha)\n",
    "    r_rw=rayleigh_channel(rcr,r,three_point_five,ninesix_point_five,i_min,alpha)\n",
    "    \n",
    "    irw=cv2.merge([b_rw,g_rw,r_rw])\n",
    "    return irw    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47c5d0",
   "metadata": {},
   "source": [
    "# Global contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c888fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forebackinit(c):\n",
    "    m=len(c)\n",
    "    n=len(c[0])\n",
    "    b = np.zeros_like(c)\n",
    "    f = np.zeros_like(c)\n",
    "    i_max=c.max()\n",
    "    i_min=c.min()\n",
    "    loss=1\n",
    "    num1=0\n",
    "    num2=0\n",
    "    s1=0\n",
    "    s2=0\n",
    "    i_init=(i_min+i_max)/2\n",
    "    while loss>=0.5:\n",
    "        for i in range(1,m):\n",
    "            for j in range(1,n):\n",
    "                if c[i,j]>i_init:\n",
    "                    num1=num1+1\n",
    "                    s1=s1+c[i,j]\n",
    "                else:\n",
    "                    num2=num2+1\n",
    "                    s2=s2+c[i,j]\n",
    "        mean_s1=s1/num1\n",
    "        mean_s2=s2/num2\n",
    "        i_new=(mean_s1+mean_s2)/2\n",
    "        loss=abs(i_init-i_new)\n",
    "        i_init=i_new\n",
    "        if loss>=0.5:\n",
    "            num1=0\n",
    "            num2=0\n",
    "            s1=0\n",
    "            s2=0\n",
    "    i_opt=i_init\n",
    "    for i in range(1,m):\n",
    "        for j in range(1,n):\n",
    "            if c[i,j]<i_opt:\n",
    "                b[i,j]=c[i,j]\n",
    "            else:\n",
    "                f[i,j]=c[i,j]\n",
    "    \n",
    "    return(b,f,i_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817d00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(image1, image2, threshold):\n",
    "\n",
    "  # Convert the images to grayscale.\n",
    "#     gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "#     gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "      # Threshold the images.\n",
    "    thresh1 = cv2.threshold(image1, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh2 = cv2.threshold(image2, threshold, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "\n",
    "      # Combine the thresholded images.\n",
    "    combined_image = cv2.bitwise_or(thresh1, thresh2)\n",
    "\n",
    "      # Return the combined image.\n",
    "\n",
    "\n",
    "    return combined_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9393f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_image(img):\n",
    "    blue,green,red = cv2.split(img)\n",
    "    bb,bf,bt=forebackinit(blue)\n",
    "    gb,gf,gt=forebackinit(green)\n",
    "    rb,rf,rt=forebackinit(red)\n",
    "    \n",
    "    t=0.067\n",
    "    combined_b=fuse_images(bb,bf,t)\n",
    "    combined_g=fuse_images(gb,gf,t)\n",
    "    combined_r=fuse_images(rb,rf,t)\n",
    "    \n",
    "    i_b=cv2.merge([bb,gb,rb])\n",
    "    i_f=cv2.merge([bf,gf,rf])\n",
    "    result=cv2.merge([combined_b,combined_g,combined_r])\n",
    "    \n",
    "    return (result,i_b,i_f)\n",
    "#     cv2.imshow(\"Background\", i_b)\n",
    "#     cv2.imshow(\"Foreground\",i_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c1e17",
   "metadata": {},
   "source": [
    "# MSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d091ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_images(image1, image2, alpha):\n",
    "    if image1.shape != image2.shape:\n",
    "        raise ValueError(\"The images must be the same size.\")\n",
    "    beta = 1.0 - alpha\n",
    "    fused_image = cv2.addWeighted(image1, beta, image2, alpha, 0,dtype=cv2.CV_32F).astype(np.uint8)\n",
    "    return fused_image    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff96991",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ca4c882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load an example image (replace with your image path)\n",
    "    \n",
    "    #image_path = r\"C:\\Users\\Megha\\Desktop\\IP\\Project\\Implementation\\images\\lol_dataset\\our485\\low\\9.png\"\n",
    "    image_path=r\"C:\\Users\\Megha\\Desktop\\IP\\Project\\Implementation\\images\\lol_dataset\\our485\\low\\131.png\"\n",
    "    #image_path = r\"C:\\Users\\Megha\\Desktop\\IP\\Project\\underwater0.jpeg\"\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    #histogram(image)\n",
    "    \n",
    "    i_max,i_int,i_min=get_channels(image)\n",
    "    \n",
    "    # Define attention weights for BGR channels (adjust these values as needed)\n",
    "    attention_weights = get_attention_weights(i_max,i_int,i_min) #[1.2, 0.8, 1.0]\n",
    "    \n",
    "\n",
    "    #Apply attention weights to the image\n",
    "    weighted_image,corrected_image = attach_attention_weights(image,attention_weights,i_max,i_int,i_min)    \n",
    "    \n",
    "    global_img,background,foreground=global_image(corrected_image)\n",
    "    \n",
    "    \n",
    "#     kernel_size = 5\n",
    "    \n",
    "#     weighted_nonoise=remove_noise(weighted_image, kernel_size)\n",
    "#     corrected_nonoise=remove_noise(corrected_image, kernel_size)\n",
    "    \n",
    "#     print(\"ICR\")\n",
    "#     histogram(corrected_nonoise)\n",
    "#     print(\"WI\")\n",
    "#     histogram(weighted_nonoise)\n",
    "    \n",
    "    rayleigh_image=rayleigh_dist(corrected_image,image,i_min,alpha=10.0)\n",
    "    fused_image=fuse_images(background,rayleigh_image,0.05)\n",
    "    \n",
    "    # Display the original and weighted images\n",
    "#     cv2.imshow(\"Color Corrected Image\", corrected_image)\n",
    "#     cv2.imshow(\"Rayleigh Image\", rayleigh_image)\n",
    "    cv2.imshow(\"Fused Image\", fused_image)\n",
    "#     cv2.imshow(\"Background Image\",background)\n",
    "#     cv2.imshow(\"Foregorund Image\",foreground)\n",
    "#     cv2.imshow(\"Rayleigh Image\",rayleigh_image)\n",
    "#     cv2.imshow(\"Corrected Image\",corrected_image)\n",
    "    \n",
    "    \n",
    "#     cv2.imshow(\"Weighted Image\", weighted_nonoise)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b5705a",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46936f01",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/13875989/comparing-image-in-url-to-image-in-filesystem-in-python/13884956#13884956"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e3936",
   "metadata": {},
   "source": [
    "## BRISQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8749e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.34065580884871\n",
      "48.73153243483941\n",
      "50.22642512638296\n",
      "39.692637804484974\n",
      "41.37039440762416\n",
      "56.650914671362756\n"
     ]
    }
   ],
   "source": [
    "from brisque import BRISQUE\n",
    "\n",
    "obj = BRISQUE(url=False)\n",
    "print(obj.score(image))\n",
    "print(obj.score(weighted_image))\n",
    "print(obj.score(rayleigh_image))\n",
    "print(obj.score(fused_image))\n",
    "print(obj.score(background))\n",
    "print(obj.score(foreground))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc31be",
   "metadata": {},
   "source": [
    "## Average Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f4a28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_gradient_metric(image):\n",
    "    # Convert the image to grayscale.\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute the gradients of the image.\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    # Calculate the magnitude of the gradients.\n",
    "    gradient_magnitude = cv2.magnitude(sobelx, sobely)\n",
    "    # Calculate the average gradient metric.\n",
    "    average_gradient_metric = np.mean(gradient_magnitude)\n",
    "    # Return the average gradient metric.\n",
    "    return average_gradient_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13035b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.44626774090773\n",
      "94.45831800720772\n",
      "8.235941968795236\n"
     ]
    }
   ],
   "source": [
    "print(calculate_average_gradient_metric(fused_image))\n",
    "print(calculate_average_gradient_metric(corrected_image))\n",
    "print(calculate_average_gradient_metric(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120920da",
   "metadata": {},
   "source": [
    "## PCQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c63780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patch(image, patch_size):\n",
    "    # Calculate the center of the patch.\n",
    "    patch_center = (patch_size[0] // 2, patch_size[1] // 2)\n",
    "    # Extract the patch from the image.\n",
    "    patch = image[patch_center[0] - patch_size[0] // 2:patch_center[0] + patch_size[0] // 2,\n",
    "                patch_center[1] - patch_size[1] // 2:patch_center[1] + patch_size[1] // 2,\n",
    "                :]\n",
    "    # Return the patch.\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ae9ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pcqi(image):\n",
    "    # Split the image into patches.\n",
    "    patches = extract_patch(image, (32, 32))\n",
    "    # Calculate the contrast of each patch.\n",
    "    patch_contrasts = []\n",
    "    for patch in patches:\n",
    "        patch_contrast = cv2.Laplacian(patch, cv2.CV_64F).var()\n",
    "        patch_contrasts.append(patch_contrast)\n",
    "    # Calculate the average patch contrast.\n",
    "    average_patch_contrast = np.mean(patch_contrasts)\n",
    "    \n",
    "    # Calculate the PCQI.\n",
    "    pcqi = average_patch_contrast / 255.0\n",
    "    # Return the PCQI.\n",
    "    return pcqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "705d6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.335035123804294\n",
      "0.2322793798508987\n"
     ]
    }
   ],
   "source": [
    "print(calculate_pcqi(fused_image))\n",
    "print(calculate_pcqi(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97083451",
   "metadata": {},
   "source": [
    "## NIQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2e36805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4a7c9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mscns(image):\n",
    "    # Calculate the local mean and standard deviation of the image.\n",
    "    local_mean = scipy.signal.convolve2d(image, np.ones((3, 3)), 'same') / 9.0\n",
    "    local_std = np.sqrt(scipy.signal.convolve2d((image - local_mean)**2, np.ones((3, 3)), 'same') / 9.0)\n",
    "    \n",
    "    zero_values = np.where(local_std == 0)\n",
    "    \n",
    "    # Replace zero values with a small positive value.\n",
    "    local_std[zero_values] = 1e-6\n",
    "    # Calculate the MSCN coefficients.\n",
    "    \n",
    "    mscns = (image - local_mean) / local_std\n",
    "    # Return the MSCN coefficients.\n",
    "    return mscns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "494beac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ggd_parameters(mscn_coefficients):\n",
    "    # Calculate the mean and standard deviation of the MSCN coefficients.\n",
    "    mean = np.mean(mscn_coefficients)\n",
    "    std = np.std(mscn_coefficients)\n",
    "    # Calculate the shape parameter of the GGD.\n",
    "    alpha = scipy.special.gamma((11.0 / 6.0)) / scipy.special.gamma((7.0 / 6.0)) * ((std / mean)**2)\n",
    "    # Calculate the GGD parameters.\n",
    "    ggd_parameters = [mean, std, alpha]\n",
    "    # Return the GGD parameters.\n",
    "    return ggd_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f425f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_niqe_score(ggd_parameters):\n",
    "    # Calculate the NIQE score.\n",
    "    niqe_score = ggd_parameters[0] * np.sqrt(ggd_parameters[2]) + 0.15 * ggd_parameters[1]\n",
    "    # Return the NIQE score.\n",
    "    return niqe_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c66ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_niqe(image):\n",
    "    # Convert the image to grayscale.\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate the locally normalized luminescence (MSCN) coefficients.\n",
    "    mscns = calculate_mscns(gray)\n",
    "    # Calculate the generalized Gaussian distribution (GGD) parameters.\n",
    "    ggd_parameters = calculate_ggd_parameters(mscns)\n",
    "    # Calculate the NIQE.\n",
    "    niqe = calculate_niqe_score(ggd_parameters)\n",
    "    # Return the NIQE.\n",
    "    return abs(niqe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2d613931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8328188976939206\n",
      "1.1450653636047683\n"
     ]
    }
   ],
   "source": [
    "print(calculate_niqe(fused_image))\n",
    "print(calculate_niqe(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f61472",
   "metadata": {},
   "source": [
    "## Information Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f508e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_entropy(image):\n",
    "    # Check for zero values in the image.\n",
    "#     zero_values = np.where(image == 0)\n",
    "    \n",
    "#     # Replace zero values with a small positive value.\n",
    "#     image[zero_values] = 1e-6\n",
    "    \n",
    "    # Calculate the histogram of the image.\n",
    "    histogram = np.histogram(image, bins=256)[0]\n",
    "    \n",
    "    # Calculate the probability of each pixel value.\n",
    "    p_pixel_values = histogram / histogram.sum()\n",
    "    \n",
    "    zero_values = np.where(p_pixel_values == 0)\n",
    "    \n",
    "    # Replace zero values with a small positive value.\n",
    "    p_pixel_values[zero_values] = 1e-6\n",
    "    \n",
    "    # Calculate the information entropy of the image.\n",
    "    information_entropy = -np.sum(p_pixel_values * np.log2(p_pixel_values))\n",
    "    # Return the information entropy of the image.\n",
    "    return information_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49cd4799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9573788706891264\n",
      "4.901051396781516\n"
     ]
    }
   ],
   "source": [
    "print(calculate_image_entropy(fused_image))\n",
    "print(calculate_image_entropy(image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
